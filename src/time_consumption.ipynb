{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from receptive_fields.models.retinal import RetinalModel\n",
    "from torchinfo import summary\n",
    "from receptive_fields.util.activation_visualization import single_effective_receptive_field, get_input_output_shape\n",
    "from matplotlib import pyplot as plt\n",
    "from receptive_fields.util.experiment_setup import load_dataset, load_model\n",
    "from receptive_fields.util.training import train, num_correct\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SimpleCNN                                [1, 10]                   --\n",
       "├─Conv2d: 1-1                            [1, 32, 51, 38]           7,808\n",
       "├─Sequential: 1-8                        --                        (recursive)\n",
       "│    └─ELU: 2-1                          [1, 32, 51, 38]           --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─Conv2d: 2-2                       [1, 32, 23, 16]           50,208\n",
       "├─Sequential: 1-8                        --                        (recursive)\n",
       "│    └─ELU: 2-3                          [1, 32, 23, 16]           --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─Conv2d: 2-4                       [1, 32, 10, 6]            25,632\n",
       "├─Sequential: 1-8                        --                        (recursive)\n",
       "│    └─ELU: 2-5                          [1, 32, 10, 6]            --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─Conv2d: 2-6                       [1, 32, 8, 4]             9,248\n",
       "├─Sequential: 1-8                        --                        (recursive)\n",
       "│    └─ELU: 2-7                          [1, 32, 8, 4]             --\n",
       "├─Sequential: 1-9                        [1, 10]                   --\n",
       "│    └─Linear: 2-8                       [1, 1024]                 1,049,600\n",
       "│    └─ELU: 2-9                          [1, 1024]                 --\n",
       "│    └─Linear: 2-10                      [1, 1024]                 1,049,600\n",
       "│    └─ELU: 2-11                         [1, 1024]                 --\n",
       "│    └─Linear: 2-12                      [1, 1024]                 1,049,600\n",
       "│    └─ELU: 2-13                         [1, 1024]                 --\n",
       "│    └─Linear: 2-14                      [1, 10]                   10,250\n",
       "├─Softmax: 1-10                          [1, 10]                   --\n",
       "==========================================================================================\n",
       "Total params: 3,251,946\n",
       "Trainable params: 3,251,946\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 38.60\n",
       "==========================================================================================\n",
       "Input size (MB): 0.23\n",
       "Forward/backward pass size (MB): 0.64\n",
       "Params size (MB): 13.01\n",
       "Estimated Total Size (MB): 13.88\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size = (160,120)#(143,107)#(179,143)#\n",
    "# retinal_base_model = RetinalModel(img_size=img_size, num_classes=10, n_base_channels=16, n_lgn_channels=32, ceil_mode=True, l1_kernel_size=3, pool=[True, True, True], spd=[1,1,1], lgn_kernel_size=3, v1_kernel_size=4, rgc_kernel_size=3, fc_act=True)\n",
    "# mod3 = RetinalModel(img_size=img_size, num_classes=10, n_base_channels=16, n_lgn_channels=32, ceil_mode=True, l1_kernel_size=7, pool=[False, False, True], spd=[2,2,1], lgn_kernel_size=7, v1_kernel_size=7, rgc_kernel_size=7, fc_act=True)\n",
    "# model = mod3\n",
    "model = load_model(\"models/incnet/stride_3_2_2_1_ks_dec\")\n",
    "\n",
    "in_channels = model.in_channels\n",
    "img_size = model.img_size\n",
    "\n",
    "train_data = load_dataset(grayscale= in_channels==1, img_size=model.img_size)\n",
    "train_loader = DataLoader(train_data, batch_size=512, shuffle=False)\n",
    "model.to(device)\n",
    "summary(model, (1,3,*img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (_activation_func): ELU(alpha=1.0, inplace=True)\n",
       "  (space_to_depth): SpaceToDepth()\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(9, 9), stride=(3, 3))\n",
       "  (softmax): Softmax(dim=-1)\n",
       "  (extra_conv_layers): ModuleList(\n",
       "    (0): Conv2d(32, 32, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): ELU(alpha=1.0, inplace=True)\n",
       "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (3): ELU(alpha=1.0, inplace=True)\n",
       "    (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (5): ELU(alpha=1.0, inplace=True)\n",
       "    (6): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "(inputs, labels) = next(iter(train_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-23 11:22:48 33767:33767 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "100%|██████████| 100/100 [00:03<00:00, 25.60it/s, Loss=4.39e+3, Acc.:=0.105]\n",
      "STAGE:2024-05-23 11:22:52 33767:33767 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-23 11:22:52 33767:33767 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.005, weight_decay=1e-6)\n",
    "with torch.autograd.profiler.profile(use_cuda=True, record_shapes=True) as prof:\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    epoch_correct = 0\n",
    "    pbar = tqdm(range(100))\n",
    "    for i in pbar:\n",
    "        # optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        batch_correct =  num_correct(outputs, labels)\n",
    "        epoch_correct += batch_correct\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        pbar.set_postfix({'Loss': loss.item(), 'Acc.:': batch_correct/len(labels)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_avgs = prof.key_averages(group_by_input_shape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------------------------------------------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls                                                                                Input Shapes  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------------------------------------------------  \n",
      "                             aten::convolution_backward         0.61%      21.310ms         1.27%      44.609ms     446.090us        1.416s        38.93%        1.426s      14.256ms           100      [[512, 32, 23, 16], [512, 32, 51, 38], [32, 32, 7, 7], [], [], [], [], [], [], [], []]  \n",
      "                             aten::convolution_backward         0.44%      15.529ms         1.06%      37.364ms     373.640us     382.948ms        10.52%     422.169ms       4.222ms           100      [[512, 32, 51, 38], [512, 3, 160, 120], [32, 3, 9, 9], [], [], [], [], [], [], [], []]  \n",
      "                                aten::cudnn_convolution         0.19%       6.535ms         0.24%       8.494ms      84.940us     299.558ms         8.23%     299.558ms       2.996ms           100                             [[512, 3, 160, 120], [32, 3, 9, 9], [], [], [], [], [], [], []]  \n",
      "                                aten::cudnn_convolution         0.18%       6.323ms         0.23%       7.901ms      79.010us     203.944ms         5.61%     203.944ms       2.039ms           100                             [[512, 32, 51, 38], [32, 32, 7, 7], [], [], [], [], [], [], []]  \n",
      "                             aten::convolution_backward         0.76%      26.614ms         1.42%      49.990ms     499.900us     156.616ms         4.30%     160.468ms       1.605ms           100       [[512, 32, 10, 6], [512, 32, 23, 16], [32, 32, 5, 5], [], [], [], [], [], [], [], []]  \n",
      "                                     aten::elu_backward         0.12%       4.152ms         0.16%       5.681ms      56.810us      81.529ms         2.24%      81.529ms     815.290us           100                                      [[512, 32, 51, 38], [], [], [], [], [512, 32, 51, 38]]  \n",
      "                                             aten::add_         0.05%       1.859ms         0.07%       2.609ms      26.090us      70.801ms         1.95%      70.801ms     708.010us           100                                                      [[512, 32, 51, 38], [1, 32, 1, 1], []]  \n",
      "                                               aten::mm         0.47%      16.480ms         0.68%      23.807ms      79.357us      58.659ms         1.61%      58.659ms     195.530us           300                                                                 [[512, 1024], [1024, 1024]]  \n",
      "                                             aten::elu_         0.13%       4.690ms         0.16%       5.497ms      54.970us      54.922ms         1.51%      54.922ms     549.220us           100                                                             [[512, 32, 51, 38], [], [], []]  \n",
      "                             aten::convolution_backward         1.01%      35.282ms         1.82%      63.778ms     637.780us      51.657ms         1.42%      64.732ms     647.320us           100         [[512, 32, 8, 4], [512, 32, 10, 6], [32, 32, 3, 3], [], [], [], [], [], [], [], []]  \n",
      "                                               aten::mm         0.35%      12.278ms         0.46%      16.187ms      53.957us      48.613ms         1.34%      48.613ms     162.043us           300                                                                  [[1024, 512], [512, 1024]]  \n",
      "                                            aten::addmm         0.41%      14.410ms         0.52%      18.228ms      60.760us      46.525ms         1.28%      46.525ms     155.083us           300                                                 [[1024], [512, 1024], [1024, 1024], [], []]  \n",
      "autograd::engine::evaluate_function: torch::autograd...         1.31%      46.003ms         4.43%     155.645ms      97.278us      40.694ms         1.12%     115.576ms      72.235us          1600                                                                                          []  \n",
      "                                              aten::sum         0.24%       8.439ms         0.34%      11.816ms     118.160us      38.921ms         1.07%      39.021ms     390.210us           100                                                             [[512, 32, 51, 38], [], [], []]  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         1.08%      37.759ms         7.60%     266.713ms     666.783us      36.291ms         1.00%     271.315ms     678.288us           400                                                                                          []  \n",
      "      autograd::engine::evaluate_function: EluBackward1         0.75%      26.188ms         2.76%      96.926ms     138.466us      23.453ms         0.64%     172.907ms     247.010us           700                                                                                          []  \n",
      "                                aten::cudnn_convolution         0.36%      12.509ms         0.39%      13.855ms     138.550us      21.954ms         0.60%      21.954ms     219.540us           100                             [[512, 32, 23, 16], [32, 32, 5, 5], [], [], [], [], [], [], []]  \n",
      "                                              aten::sum         0.41%      14.412ms         0.53%      18.662ms      62.207us      21.951ms         0.60%      21.951ms      73.170us           300                                                                   [[512, 1024], [], [], []]  \n",
      "                                     aten::elu_backward         0.35%      12.222ms         0.49%      17.188ms      57.293us      20.638ms         0.57%      20.638ms      68.793us           300                                                  [[512, 1024], [], [], [], [], [512, 1024]]  \n",
      "                                                aten::t         0.68%      23.741ms         1.91%      66.897ms      55.748us      19.582ms         0.54%      47.677ms      39.731us          1200                                                                              [[1024, 1024]]  \n",
      "                                        aten::transpose         0.67%      23.367ms         1.00%      35.130ms      29.275us      19.499ms         0.54%      28.095ms      23.413us          1200                                                                      [[1024, 1024], [], []]  \n",
      "                                         AddmmBackward0         0.92%      32.374ms         4.07%     142.996ms     476.653us      15.969ms         0.44%     149.693ms     498.977us           300                                                                               [[512, 1024]]  \n",
      "        autograd::engine::evaluate_function: TBackward0         0.35%      12.129ms         1.44%      50.523ms     126.308us      15.621ms         0.43%      54.734ms     136.835us           400                                                                                          []  \n",
      "                                     aten::elu_backward         0.12%       4.242ms         0.16%       5.781ms      57.810us      15.487ms         0.43%      15.487ms     154.870us           100                                      [[512, 32, 23, 16], [], [], [], [], [512, 32, 23, 16]]  \n",
      "                                aten::_foreach_addcdiv_         0.44%      15.583ms         0.59%      20.775ms     207.750us      14.596ms         0.40%      16.196ms     161.960us           100                                                                            [[], [], [], []]  \n",
      "                                     aten::_foreach_add         1.01%      35.432ms         1.71%      60.058ms     600.580us      14.435ms         0.40%      17.647ms     176.470us           100                                                                                [[], [], []]  \n",
      "                                             aten::add_         0.04%       1.543ms         0.06%       2.132ms      21.320us      13.708ms         0.38%      13.708ms     137.080us           100                                                      [[512, 32, 23, 16], [1, 32, 1, 1], []]  \n",
      "                                aten::_foreach_addcmul_         0.45%      15.642ms         0.60%      20.944ms     209.440us      11.751ms         0.32%      13.359ms     133.590us           100                                                                            [[], [], [], []]  \n",
      "                                         AddmmBackward0         0.31%      10.991ms         1.34%      47.141ms     471.410us      10.841ms         0.30%      48.143ms     481.430us           100                                                                                 [[512, 10]]  \n",
      "                                             aten::elu_         0.07%       2.443ms         0.09%       3.149ms      31.490us      10.638ms         0.29%      10.638ms     106.380us           100                                                             [[512, 32, 23, 16], [], [], []]  \n",
      "                                              aten::sum         0.29%      10.004ms         0.36%      12.691ms     126.910us      10.367ms         0.28%      11.739ms     117.390us           100                                                               [[512, 32, 8, 4], [], [], []]  \n",
      "                                           EluBackward1         0.23%       7.950ms         0.79%      27.876ms      92.920us      10.294ms         0.28%      30.932ms     103.107us           300                                                                               [[512, 1024]]  \n",
      "                                aten::cudnn_convolution         0.18%       6.463ms         0.22%       7.869ms      78.690us      10.270ms         0.28%      10.270ms     102.700us           100                              [[512, 32, 10, 6], [32, 32, 3, 3], [], [], [], [], [], [], []]  \n",
      "                                    aten::_foreach_sqrt         0.55%      19.398ms         1.06%      37.303ms     373.030us       9.058ms         0.25%      10.658ms     106.580us           100                                                                                        [[]]  \n",
      "                                              aten::sum         0.25%       8.620ms         0.35%      12.158ms     121.580us       8.926ms         0.25%       9.026ms      90.260us           100                                                             [[512, 32, 23, 16], [], [], []]  \n",
      "                        torch::autograd::AccumulateGrad         0.19%       6.650ms         0.54%      19.014ms      63.380us       8.920ms         0.25%      21.854ms      72.847us           300                                                                                    [[1024]]  \n",
      "                                    aten::_foreach_add_         0.43%      14.967ms         0.58%      20.195ms     201.950us       8.899ms         0.24%      10.499ms     104.990us           100                                                                                    [[], []]  \n",
      "                                    aten::_foreach_mul_         0.47%      16.662ms         0.63%      22.073ms     220.730us       8.861ms         0.24%      10.461ms     104.610us           100                                                                                    [[], []]  \n",
      "                                       aten::as_strided         0.10%       3.440ms         0.10%       3.440ms       2.867us       8.596ms         0.24%       8.596ms       7.163us          1200                                                                  [[1024, 1024], [], [], []]  \n",
      "                                               aten::mm         0.16%       5.518ms         0.21%       7.255ms      72.550us       8.503ms         0.23%       8.503ms      85.030us           100                                                                     [[512, 10], [10, 1024]]  \n",
      "                        torch::autograd::AccumulateGrad         0.18%       6.187ms         0.50%      17.676ms      58.920us       8.448ms         0.23%      20.481ms      68.270us           300                                                                              [[1024, 1024]]  \n",
      "                                aten::nll_loss_backward         0.18%       6.381ms         0.47%      16.560ms     165.600us       8.379ms         0.23%      17.568ms     175.680us           100                                                      [[], [512, 10], [512], [], [], [], []]  \n",
      "autograd::engine::evaluate_function: ConvolutionBack...         0.52%      18.362ms         6.65%     233.381ms     583.452us       8.280ms         0.23%        2.089s       5.223ms           400                                                                                          []  \n",
      "                                              aten::sum         0.14%       5.088ms         0.19%       6.653ms      66.530us       7.790ms         0.21%       7.790ms      77.900us           100                                                                     [[512, 10], [], [], []]  \n",
      "                                           aten::detach         0.16%       5.653ms         0.29%      10.052ms      33.507us       7.753ms         0.21%      12.934ms      43.113us           300                                                                                    [[1024]]  \n",
      "                                     aten::elu_backward         0.12%       4.367ms         0.17%       5.973ms      59.730us       7.621ms         0.21%       7.621ms      76.210us           100                                        [[512, 32, 10, 6], [], [], [], [], [512, 32, 10, 6]]  \n",
      "                                             TBackward0         0.15%       5.382ms         0.75%      26.367ms      87.890us       7.611ms         0.21%      29.167ms      97.223us           300                                                                              [[1024, 1024]]  \n",
      "                                           aten::detach         0.15%       5.288ms         0.26%       9.207ms      30.690us       7.381ms         0.20%      12.033ms      40.110us           300                                                                              [[1024, 1024]]  \n",
      "                              aten::_local_scalar_dense         0.10%       3.533ms        61.36%        2.154s       7.179ms       7.004ms         0.19%       7.004ms      23.347us           300                                                                                        [[]]  \n",
      "                                               aten::mm         0.12%       4.113ms         0.16%       5.470ms      54.700us       6.789ms         0.19%       6.789ms      67.890us           100                                                                    [[10, 512], [512, 1024]]  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------------------------------------------------------------------------------------  \n",
      "Self CPU time total: 3.510s\n",
      "Self CUDA time total: 3.638s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(key_avgs.table(sort_by=\"self_cuda_time_total\", row_limit=50, max_shapes_column_width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aaa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aaa\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aaa' is not defined"
     ]
    }
   ],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             aten::convolution_backward         1.43%     102.897ms         2.37%     170.564ms     426.410us        2.227s        30.21%        2.498s       6.245ms           400  \n",
      "                              aten::avg_pool2d_backward         0.46%      32.970ms         0.51%      36.898ms     184.490us        1.722s        23.35%        1.722s       8.608ms           200  \n",
      "                                aten::cudnn_convolution         0.47%      33.555ms         0.61%      43.808ms     109.520us     660.918ms         8.96%     660.918ms       1.652ms           400  \n",
      "                                     aten::elu_backward         0.17%      12.138ms         0.25%      17.885ms      44.712us     597.310ms         8.10%     597.310ms       1.493ms           400  \n",
      "                                             aten::add_         0.11%       8.214ms         0.17%      12.448ms      31.120us     497.875ms         6.75%     497.875ms       1.245ms           400  \n",
      "                                             aten::elu_         0.17%      12.398ms         0.22%      15.962ms      39.905us     404.130ms         5.48%     404.130ms       1.010ms           400  \n",
      "                                       aten::avg_pool2d         0.09%       6.156ms         0.12%       8.329ms      41.645us     376.065ms         5.10%     376.065ms       1.880ms           200  \n",
      "                                              aten::sum         0.66%      47.540ms         0.97%      69.608ms      87.010us     290.343ms         3.94%     292.533ms     365.666us           800  \n",
      "                 aten::max_pool2d_with_indices_backward         1.22%      88.007ms         1.31%      94.302ms     943.020us     127.291ms         1.73%     133.953ms       1.340ms           100  \n",
      "                                               aten::mm         0.29%      21.104ms         0.40%      28.943ms      48.238us      37.183ms         0.50%      37.183ms      61.972us           600  \n",
      "                                        aten::transpose         0.48%      34.324ms         0.53%      37.877ms      25.251us      29.932ms         0.41%      42.663ms      28.442us          1500  \n",
      "                                                aten::t         0.49%      35.477ms         1.02%      73.354ms      48.903us      28.915ms         0.39%      71.578ms      47.719us          1500  \n",
      "                          MaxPool2DWithIndicesBackward0         0.35%      24.875ms         1.66%     119.177ms       1.192ms      24.273ms         0.33%     158.226ms       1.582ms           100  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.38%      27.550ms         2.14%     153.782ms     512.607us      23.103ms         0.31%     157.604ms     525.347us           300  \n",
      "                                         AddmmBackward0         0.50%      35.872ms         1.50%     108.164ms     360.547us      22.617ms         0.31%     110.722ms     369.073us           300  \n",
      "autograd::engine::evaluate_function: torch::autograd...         0.57%      40.804ms         1.53%     109.764ms      78.403us      21.637ms         0.29%      60.416ms      43.154us          1400  \n",
      "                              aten::_local_scalar_dense         0.06%       4.491ms        81.71%        5.875s      19.583ms      18.187ms         0.25%      18.187ms      60.623us           300  \n",
      "                                     aten::_log_softmax         0.07%       4.988ms         0.10%       7.120ms      71.200us      17.523ms         0.24%      17.523ms     175.230us           100  \n",
      "                        torch::autograd::AccumulateGrad         0.44%      31.983ms         0.96%      68.960ms      49.257us      15.768ms         0.21%      38.779ms      27.699us          1400  \n",
      "                                           aten::detach         0.40%      29.052ms         0.51%      36.977ms      26.412us      13.572ms         0.18%      23.011ms      16.436us          1400  \n",
      "                                       aten::as_strided         0.07%       5.280ms         0.07%       5.280ms       2.400us      13.470ms         0.18%      13.470ms       6.123us          2200  \n",
      "                                     AvgPool2DBackward0         0.25%      18.268ms         0.77%      55.166ms     275.830us      12.551ms         0.17%        1.734s       8.671ms           200  \n",
      "                                            aten::fill_         0.06%       4.588ms         0.13%       9.123ms      29.054us      11.673ms         0.16%      11.673ms      37.175us           314  \n",
      "                          aten::max_pool2d_with_indices         0.06%       4.033ms         0.07%       5.248ms      52.480us      10.498ms         0.14%      10.498ms     104.980us           100  \n",
      "                                                 detach         0.11%       7.925ms         0.11%       7.925ms       5.661us       9.439ms         0.13%       9.439ms       6.742us          1400  \n",
      "                                 aten::nll_loss_forward         0.08%       5.451ms         0.09%       6.602ms      66.020us       8.486ms         0.12%      11.219ms     112.190us           100  \n",
      "        autograd::engine::evaluate_function: TBackward0         0.11%       8.225ms         0.41%      29.440ms      98.133us       7.782ms         0.11%      32.327ms     107.757us           300  \n",
      "                                      aten::convolution         0.25%      17.739ms         1.55%     111.374ms     278.435us       7.540ms         0.10%        1.173s       2.933ms           400  \n",
      "                                      aten::result_type         0.00%     248.000us         0.00%     248.000us       0.035us       7.346ms         0.10%       7.346ms       1.049us          7000  \n",
      "                                aten::nll_loss_backward         0.08%       5.902ms         0.19%      13.753ms     137.530us       7.058ms         0.10%      13.679ms     136.790us           100  \n",
      "autograd::engine::evaluate_function: LogSoftmaxBackw...         0.08%       5.780ms         0.19%      13.364ms     133.640us       5.858ms         0.08%      13.689ms     136.890us           100  \n",
      "                                            aten::addmm         0.36%      25.919ms         0.46%      32.898ms     109.660us       5.812ms         0.08%       5.812ms      19.373us           300  \n",
      "                                     aten::_foreach_add         0.66%      47.237ms         0.82%      59.162ms     591.620us       5.746ms         0.08%       8.747ms      87.470us           100  \n",
      "                                             TBackward0         0.09%       6.343ms         0.30%      21.215ms      70.717us       5.717ms         0.08%      24.545ms      81.817us           300  \n",
      "                                             aten::view         0.07%       4.984ms         0.07%       4.984ms       5.538us       5.705ms         0.08%       5.705ms       6.339us           900  \n",
      "                                             aten::item         0.15%      10.745ms        81.86%        5.886s      19.618ms       5.608ms         0.08%      23.795ms      79.317us           300  \n",
      "                                     aten::_convolution         0.40%      29.036ms         1.30%      93.635ms     234.088us       5.493ms         0.07%        1.166s       2.914ms           400  \n",
      "                       aten::_log_softmax_backward_data         0.04%       3.025ms         0.06%       4.346ms      43.460us       4.555ms         0.06%       4.555ms      45.550us           100  \n",
      "                                    aten::empty_strided         0.32%      22.829ms         0.32%      22.829ms       7.574us       4.411ms         0.06%       4.411ms       1.464us          3014  \n",
      "autograd::engine::evaluate_function: NllLossBackward...         0.06%       4.532ms         0.31%      22.016ms     220.160us       4.124ms         0.06%      21.454ms     214.540us           100  \n",
      "                                aten::_foreach_addcdiv_         0.30%      21.590ms         0.31%      22.465ms     224.650us       3.781ms         0.05%       5.220ms      52.200us           100  \n",
      "                                            aten::zero_         0.08%       6.000ms         0.17%      12.571ms      58.743us       3.769ms         0.05%      13.350ms      62.383us           214  \n",
      "                                    aten::_foreach_sqrt         0.36%      25.762ms         0.51%      36.944ms     369.440us       3.685ms         0.05%       5.198ms      51.980us           100  \n",
      "                                       NllLossBackward0         0.05%       3.731ms         0.24%      17.484ms     174.840us       3.651ms         0.05%      17.330ms     173.300us           100  \n",
      "                                    aten::_foreach_mul_         0.30%      21.801ms         0.32%      22.972ms     229.720us       3.433ms         0.05%       4.908ms      49.080us           100  \n",
      "                                aten::_foreach_addcmul_         0.31%      22.086ms         0.32%      23.059ms     230.590us       3.344ms         0.05%       4.839ms      48.390us           100  \n",
      "                                           aten::conv2d         0.13%       9.403ms         1.68%     120.777ms     301.942us       3.295ms         0.04%        1.177s       2.941ms           400  \n",
      "                                    LogSoftmaxBackward0         0.05%       3.238ms         0.11%       7.584ms      75.840us       3.276ms         0.04%       7.831ms      78.310us           100  \n",
      "                                    aten::_foreach_add_         0.29%      20.893ms         0.30%      21.809ms     218.090us       3.207ms         0.04%       4.671ms      46.710us           100  \n",
      "                                      aten::log_softmax         0.06%       4.308ms         0.16%      11.474ms     114.740us       3.116ms         0.04%      21.366ms     213.660us           100  \n",
      "                                          aten::reshape         0.13%       9.347ms         0.16%      11.201ms      22.402us       3.087ms         0.04%       4.698ms       9.396us           500  \n",
      "                                          aten::resize_         0.00%     126.000us         0.00%     126.000us       1.260us       2.733ms         0.04%       2.733ms      27.330us           100  \n",
      "     autograd::engine::evaluate_function: ViewBackward0         0.04%       2.629ms         0.11%       7.780ms      77.800us       2.718ms         0.04%       8.645ms      86.450us           100  \n",
      "                                          ViewBackward0         0.03%       2.293ms         0.07%       5.151ms      51.510us       2.676ms         0.04%       5.927ms      59.270us           100  \n",
      "                               aten::cross_entropy_loss         0.06%       4.218ms         0.37%      26.535ms     265.350us       2.327ms         0.03%      39.109ms     391.090us           100  \n",
      "                                         aten::nll_loss         0.03%       2.320ms         0.12%       8.922ms      89.220us       2.300ms         0.03%      13.519ms     135.190us           100  \n",
      "                                      aten::nll_loss_nd         0.03%       1.921ms         0.15%      10.843ms     108.430us       1.897ms         0.03%      15.416ms     154.160us           100  \n",
      "                                       aten::empty_like         0.03%       2.484ms         0.05%       3.810ms      33.421us       1.656ms         0.02%       2.920ms      25.614us           114  \n",
      "                                        aten::ones_like         0.05%       3.502ms         0.13%       9.279ms      92.790us       1.582ms         0.02%       6.546ms      65.460us           100  \n",
      "                                               aten::to         0.04%       2.576ms         0.15%      11.055ms      18.425us       1.456ms         0.02%       2.830ms       4.717us           600  \n",
      "                                           aten::linear         0.19%      13.607ms         0.86%      61.638ms     205.460us       1.289ms         0.02%       8.929ms      29.763us           300  \n",
      "                            Optimizer.step#RMSprop.step         0.89%      64.215ms         3.51%     252.417ms       2.524ms       1.249ms         0.02%      35.007ms     350.070us           100  \n",
      "                                           EluBackward1         0.17%      12.551ms         0.42%      30.436ms      76.090us       1.147ms         0.02%     598.457ms       1.496ms           400  \n",
      "autograd::engine::evaluate_function: MaxPool2DWithIn...         0.05%       3.794ms         1.71%     122.971ms       1.230ms       1.135ms         0.02%     159.361ms       1.594ms           100  \n",
      "autograd::engine::evaluate_function: ConvolutionBack...         0.21%      15.438ms         2.77%     199.117ms     497.793us       1.132ms         0.02%        2.500s       6.251ms           400  \n",
      "                                              aten::max         0.12%       8.388ms         0.14%       9.749ms      97.490us       1.115ms         0.02%       1.323ms      13.230us           100  \n",
      "      autograd::engine::evaluate_function: EluBackward1         0.19%      13.371ms         0.61%      43.807ms     109.517us       1.061ms         0.01%     599.518ms       1.499ms           400  \n",
      "                                   ConvolutionBackward0         0.18%      13.115ms         2.55%     183.679ms     459.197us       1.046ms         0.01%        2.499s       6.248ms           400  \n",
      "                                            aten::copy_         0.02%       1.647ms         0.04%       2.797ms      27.970us     837.000us         0.01%     837.000us       8.370us           100  \n",
      "autograd::engine::evaluate_function: AvgPool2DBackwa...         0.09%       6.773ms         0.86%      61.939ms     309.695us     525.000us         0.01%        1.735s       8.673ms           200  \n",
      "                                            aten::empty         0.07%       5.359ms         0.07%       5.359ms      13.398us     455.000us         0.01%     455.000us       1.137us           400  \n",
      "                                         aten::_to_copy         0.06%       4.504ms         0.12%       8.479ms      84.790us     431.000us         0.01%       1.374ms      13.740us           100  \n",
      "                                               aten::eq         0.03%       2.447ms         0.05%       3.396ms      33.960us     300.000us         0.00%     300.000us       3.000us           100  \n",
      "                                       aten::max_pool2d         0.03%       2.507ms         0.11%       7.755ms      77.550us     254.000us         0.00%      10.752ms     107.520us           100  \n",
      "                                          aten::flatten         0.03%       2.171ms         0.05%       3.357ms      33.570us     246.000us         0.00%     352.000us       3.520us           100  \n",
      "                                       aten::zeros_like         0.01%     581.000us         0.02%       1.791ms     127.929us      60.000us         0.00%     175.000us      12.500us            14  \n",
      "                                  cudaStreamIsCapturing         0.02%       1.105ms         0.02%       1.105ms       1.001us       0.000us         0.00%       0.000us       0.000us          1104  \n",
      "                                  cudaStreamGetPriority         0.01%     863.000us         0.01%     863.000us       0.785us       0.000us         0.00%       0.000us       0.000us          1100  \n",
      "                       cudaDeviceGetStreamPriorityRange         0.01%     794.000us         0.01%     794.000us       0.722us       0.000us         0.00%       0.000us       0.000us          1100  \n",
      "                                       cudaLaunchKernel         1.24%      89.405ms         1.24%      89.405ms      11.297us       0.000us         0.00%       0.000us       0.000us          7914  \n",
      "                                        cudaMemsetAsync         0.09%       6.396ms         0.09%       6.396ms       5.815us       0.000us         0.00%       0.000us       0.000us          1100  \n",
      "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.03%       1.970ms         0.03%       1.970ms       0.934us       0.000us         0.00%       0.000us       0.000us          2110  \n",
      "                                        cudaMemcpyAsync        81.62%        5.868s        81.62%        5.868s      19.560ms       0.000us         0.00%       0.000us       0.000us           300  \n",
      "                                  cudaStreamSynchronize         0.03%       2.376ms         0.03%       2.376ms       7.920us       0.000us         0.00%       0.000us       0.000us           300  \n",
      "                                             cudaMalloc         0.05%       3.343ms         0.05%       3.343ms     835.750us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                                    cudaStreamWaitEvent         0.03%       2.243ms         0.03%       2.243ms       1.319us       0.000us         0.00%       0.000us       0.000us          1700  \n",
      "                                  cudaDeviceSynchronize         0.00%      37.000us         0.00%      37.000us      37.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 7.190s\n",
      "Self CUDA time total: 7.374s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avgs = prof.key_averages()\n",
    "print(avgs.table(sort_by=\"self_cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1505"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgs[index_conv].self_cuda_time_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../performance/{}_{}_resnet18/\".format(*img_size)\n",
    "if not os.path.exists(base_path):\n",
    "    os.mkdir(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.export_chrome_trace(base_path + \"trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path+\"key_averages.csv\", \"w+t\") as f:\n",
    "    f.write(key_avgs.table(sort_by=\"self_cuda_time_total\", row_limit=50, max_shapes_column_width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/gpfs01/berens/user/fseel/projects/receptive-fields/src/time_consumption.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgber4/gpfs01/berens/user/fseel/projects/receptive-fields/src/time_consumption.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(model\u001b[39m.\u001b[39mretina[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight[\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/miniconda3/envs/receptive-fields/lib/python3.11/site-packages/matplotlib/pyplot.py:2695\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2689\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[1;32m   2690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[1;32m   2691\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2692\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2693\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[1;32m   2694\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2695\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39mimshow(\n\u001b[1;32m   2696\u001b[0m         X, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm, aspect\u001b[39m=\u001b[39maspect,\n\u001b[1;32m   2697\u001b[0m         interpolation\u001b[39m=\u001b[39minterpolation, alpha\u001b[39m=\u001b[39malpha, vmin\u001b[39m=\u001b[39mvmin,\n\u001b[1;32m   2698\u001b[0m         vmax\u001b[39m=\u001b[39mvmax, origin\u001b[39m=\u001b[39morigin, extent\u001b[39m=\u001b[39mextent,\n\u001b[1;32m   2699\u001b[0m         interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   2700\u001b[0m         filternorm\u001b[39m=\u001b[39mfilternorm, filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   2701\u001b[0m         url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}),\n\u001b[1;32m   2702\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   2703\u001b[0m     sci(__ret)\n\u001b[1;32m   2704\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/miniconda3/envs/receptive-fields/lib/python3.11/site-packages/matplotlib/__init__.py:1446\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1444\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1445\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1446\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(sanitize_sequence, args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1448\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1449\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1450\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/miniconda3/envs/receptive-fields/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5663\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5656\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[1;32m   5657\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[1;32m   5658\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[1;32m   5659\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   5660\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5661\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5663\u001b[0m im\u001b[39m.\u001b[39mset_data(X)\n\u001b[1;32m   5664\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5665\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5666\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/receptive-fields/lib/python3.11/site-packages/matplotlib/image.py:697\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(A, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage):\n\u001b[1;32m    696\u001b[0m     A \u001b[39m=\u001b[39m pil_to_array(A)  \u001b[39m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 697\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39msafe_masked_invalid(A, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    699\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39muint8 \u001b[39mand\u001b[39;00m\n\u001b[1;32m    700\u001b[0m         \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mcan_cast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, \u001b[39mfloat\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msame_kind\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m    701\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage data of dtype \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m cannot be converted to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype))\n",
      "File \u001b[0;32m~/miniconda3/envs/receptive-fields/lib/python3.11/site-packages/matplotlib/cbook/__init__.py:714\u001b[0m, in \u001b[0;36msafe_masked_invalid\u001b[0;34m(x, copy)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msafe_masked_invalid\u001b[39m(x, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 714\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(x, subok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m    715\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39misnative:\n\u001b[1;32m    716\u001b[0m         \u001b[39m# If we have already made a copy, do the byteswap in place, else make a\u001b[39;00m\n\u001b[1;32m    717\u001b[0m         \u001b[39m# copy with the byte order swapped.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mbyteswap(inplace\u001b[39m=\u001b[39mcopy)\u001b[39m.\u001b[39mnewbyteorder(\u001b[39m'\u001b[39m\u001b[39mN\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Swap to native order.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/receptive-fields/lib/python3.11/site-packages/torch/_tensor.py:1030\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m   1029\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1030\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m   1031\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(model.retina[0].weight[2,0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/gpfs01/berens/user/fseel/projects/receptive-fields/src/time_consumption.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgber4/gpfs01/berens/user/fseel/projects/receptive-fields/src/time_consumption.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m combined_model\u001b[39m=\u001b[39mtrained_model\u001b[39m.\u001b[39mget_sequential()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
     ]
    }
   ],
   "source": [
    "combined_model=trained_model.get_sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model[2]=model.get_sequential()[2]\n",
    "combined_model[5]=model.get_sequential()[5]\n",
    "combined_model[10]=model.get_sequential()[10]\n",
    "combined_model[12]=model.get_sequential()[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "tensor(0.) tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs01/berens/user/fseel/miniconda3/envs/receptive-fields/lib/python3.11/site-packages/matplotlib/cm.py:478: RuntimeWarning: invalid value encountered in cast\n",
      "  xx = (xx * 255).astype(np.uint8)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from util.activation_visualization import sum_collapse_output, rescaleZeroOne\n",
    "cur_model = combined_model\n",
    "cur_img_size = img_size#(16,12)#\n",
    "out = cur_model(torch.rand((1,3,*cur_img_size)).to(device))\n",
    "print(out.shape)\n",
    "out = torch.full_like(sum_collapse_output(out), 0.2)\n",
    "print(out.shape)\n",
    "eff_rf = single_effective_receptive_field(cur_model, output_signal=out.to(device), input_size=(3,*cur_img_size), n_batch=1, device=device)\n",
    "print(eff_rf.min(), eff_rf.max())\n",
    "plt.imshow(torch.swapdims(rescaleZeroOne(eff_rf), 0,2))\n",
    "# plt.imshow((eff_rf!=0).numpy()[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetinalModel(\n",
       "  (retina): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): AvgPool2d(kernel_size=3, stride=3, padding=0)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): AvgPool2d(kernel_size=3, stride=3, padding=0)\n",
       "    (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (7): ELU(alpha=1.0)\n",
       "    (8): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (9): ELU(alpha=1.0)\n",
       "    (10): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=True)\n",
       "    (11): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "receptive-fields",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
