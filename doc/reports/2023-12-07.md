# Update 2023-12-07

## Regularization

- helps getting the first layer to train -> meaningful representations
- if strong regularization: simple center surround shape in first layer, 2 (on-off, off-on) in 2nd, 3rd gabors etc?
  - strong regularization: decrease train/test gap (basically increase regularization until train perf is close to test perf)
  - Testset: 60-70%, Trainset: 100% (unregularized)
  - Settings: simple CNN ((9x9 Conv -> GELU )x4 -> FC for prediction)

### Classic weight decay & momentum

Testset: ~65%, Trainset: 68%

Layer 1
![](../../imgs/regularization/weight_decay_1e-2_momentum0.9_l1.gif)
Layer 2
![](../../imgs/regularization/weight_decay_1e-2_momentum0.9_l2.gif)
Layer 3
![](../../imgs/regularization/weight_decay_1e-2_momentum0.9_l3.gif)
Layer 4
![](../../imgs/regularization/weight_decay_1e-2_momentum0.9_l4.gif)

- "Problem": Weight decay under influence of learning rate & momentum (AdamW / SGDW paper)

### Weight regularization

#### L2

Testset: ~60%, Trainset: 89% (aborted/early stop)

Layer 1
![](../../imgs/regularization/l2_weightreg_2e-4_l1.gif)
Layer 2
![](../../imgs/regularization/l2_weightreg_2e-4_l2.gif)
Layer 3
![](../../imgs/regularization/l2_weightreg_2e-4_l3.gif)
Layer 4
![](../../imgs/regularization/l2_weightreg_2e-4_l4.gif)

#### L1

Testset: ~61%, Trainset: 91% (aborted/early stop)

Layer 1
![](../../imgs/regularization/l1_weightreg_1e-6_l1.gif)
Layer 2
![](../../imgs/regularization/l1_weightreg_1e-6_l2.gif)
Layer 3
![](../../imgs/regularization/l1_weightreg_1e-6_l3.gif)
Layer 4
![](../../imgs/regularization/l1_weightreg_1e-6_l4.gif)

### Activation regularization

(after activation function)

! minimal weight decay active (1e-6 l2, forgot to turn off default)

#### L2

Testset: ~73%, Trainset: 100%

Layer 1
![](../../imgs/regularization/l2_actreg_1e-6_l1.gif)
Layer 2
![](../../imgs/regularization/l2_actreg_1e-6_l2.gif)
Layer 3
![](../../imgs/regularization/l2_actreg_1e-6_l3.gif)
Layer 4
![](../../imgs/regularization/l2_actreg_1e-6_l4.gif)

#### L1

Testset: ~64%, Trainset: 100%

Layer 1
![](../../imgs/regularization/l1_actreg_1e-7_l1.gif)
Layer 2
![](../../imgs/regularization/l1_actreg_1e-7_l2.gif)
Layer 3
![](../../imgs/regularization/l1_actreg_1e-7_l3.gif)
Layer 4
![](../../imgs/regularization/l1_actreg_1e-7_l4.gif)


### Additional stuff

- construct network with only the number of channels supposedly needed
  - somewhat hard to train (random initialization might be unfortunate)
  - but similar structures appear without regularization

### Preliminary Conclusions

- weight reg gives "nicer" RFs
- under strong regulation: "grayscale" filters
- act reg gives better performance
- act reg use more color

## Outlook

- effect of momentum
  - if weight reg is decoupled, still same results?
- why act reg l2 so much better performing?
  - rfs not that interpretable
- change task / make task more complex
  - how does it affect the rfs?
  - eg add "localization" as taskt as in RL setting, scale & shift transform