# Update 2024-04-30

## RF-Project
- made captum run / compatibility with models
  - rf idea to think about: What happens when pooling layers/stride are dropped in hindsight? Effect on RFs?
    - looks like somewhat "nicer" gabors can actually be seen for models trained with pooling/stride
  - "intermediate rfs" - say layer 2-4 instead of always back to input?
    - problematic: too many input channels
    - first glance not too promising
- brainscore
  - just a sidenote: Might be interesting for AREADNE Poster (does the brainscore improve with nicer representations?)
  - painfully long time to compute (overnight run)?
  - need to look more into which benchmark is relevant for us + exact definition
  - problem: Influence of input size?

## Back into RL land
- Main README talking about docker, but you switched to singularity, right? (went with the conda setup so far)
- Purpose of Splitting model in Encoder / Core / Decoder (what's the meaningful use case of that)?
- at least the main "functions" (train, analyze, enjoy) a somewhat more detailed documentation of how to use would be helpful
  - in particular, which arguments can & need to be passed to which
  - also a small trained model somewhere to be loaded to get stuff running and test env fast?